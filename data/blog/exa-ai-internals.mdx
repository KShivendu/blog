---
title: 'Exa: Napkin Maths Behind the Google for AI'
date: '2025-09-16'
lastmod: '2025-10-02'
tags: ['search', 'internals', 'infra']
draft: true
summary: ''
images: ['/static/images/blogs/exa.png']
authors: ['default']
---

[Exa](https://exa.ai/) is a cool search engine company. They recently [raised 85M\$ for Series B](https://exa.ai/blog/announcing-series-b). There are multiple parts of Exa AI architecture that they have described in their [blogs](https://exa.ai/blog). I've compiled them together and added some of my own notes and **lots of assumptions** for infra cost [napkin maths](https://www.youtube.com/watch?v=IxkSlnrRFqc) as a fun exercise for myself. Please take my estimates with a grain of salt and let me know if I made any wrong or suboptimal assumptions :D

## üîç Lexical/Keyword search (BM25):

- They use the standard [BM25 algorithm](https://www.youtube.com/watch?v=ziiF1eFM3_4&t=698s) for lexical search.
- Initially, this required about 1.8 TB RAM (posting lists + frequency stats) for 1B docs
- After optimizations in [this post](https://exa.ai/blog/bm25-optimization) it dropped to 1.2TB (-50%) RAM for 1B docs

### Napkin maths

- I'm assuming the index is also kept in RAM for faster querying and I'm using rates from [this](https://github.com/sirupsen/napkin-math)
- RAM Cost: 1200 GB √ó \$ 2 / GB = $2400 / month
- Disk Cost: 1200 GB √ó \$ 0.35 / GB = $420 / month
- Total ~= 2800 \$ / month for 1B docs

## üß≠ Vector search:

- They train [Matryoshka embeddings](https://huggingface.co/blog/matryoshka) with 4096 dims but only keep 256 dims in RAM (i.e. MRL truncation)
- They use [binary quantization](https://qdrant.tech/articles/binary-quantization) (BQ) and some clever CPU tricks for vector distance calculations ([SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) + precomputing possible outputs and keeping them in CPU registers for skipping calculation).
- They use [IVF Index](https://youtu.be/chz74Mtd1AA?si=DvvDE3rO9HvVvox7&t=429) (also used in FAISS & pgvector) which an Approximate Nearest Neighbours Algorithm. It basically does k-means clustering, and during search time, finds the relevant clusters by comparing distance with respective cluster centroids, and only queries few of those clusters.
- Lastly, they do re-ranking with original vectors to compensate for all this approximation/compression.

### Napkin maths

- I think their model is a fine-tuned version of [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B) since it fits the description well and it doesn't make sense for such a young startup to start training embeddings from scratch.
- Qwen3-Embedding-8B has a massive context length of 32K tokens. As per this quick [research](https://www.perplexity.ai/search/average-words-per-page-on-the-qjaJmyDVQ823yG.lh.Ba3g#0), most internet pages and even research papers would fit in this context length. So chunking isn't required. 1 document = 1 vector.
- Each uncompressed vector would take 4096 dims \* 2 bytes (assuming fp16) = 8KB by default. This means 1B vectors would take 8 \* 10^3 bytes \* 10^9 = 8 \* 10^12 = 8 TB for 1B docs.
  - By truncating matryoshka it becomes, 8 / (4096 / 256) TB = 8 / 16 TB = 512 GB for 1B vectors.
  - With binary quantization, 16 bits (since fp16) becomes 1 bit. So 512 GB / 16 = 32 GB per 1B vectors (i.e. 16\*16=256x reduction)
  - Their vector search might be have these components:
    - Original vectors:
      - Disk cost: 8 \* 1e3 \* 0.35 \$ = 2.8k \$ / month for 1B docs
      - Memory cost: 8 \* 1e3 \* 2 \$ = 16k \$ / month for 1B docs. But this sounds too costly and Pareto principle says 20% of docs will account for 80% queries so let's cache only 25% in RAM and hence it should consume 2TB RAM and cost 4k \$ / month for 1B docs.
      - Total: 2.8 + 4k \$ = 6.8k \$
    - IVF index with BQ + MRL truncation + re-ranking with original vectors for their "neural search"
      - IVF Index needs centroids along with original vectors, I'm assuming they have 10B vectors and they have 100K clusters, each having 100K vectors.
        - Each cluster will have a centroid and I'm assuming they keep uncompressed centroid vectors for maximizing recall.
        - Centroids will require 100K \* 4096 \* 2 bytes = 820GB for 10B vectors (or 82GB for 1B vectors)
        - Each vector ID needs to stored in the IVF Index, 10B \* 8 bytes = 80 GB (assuming that for point id they'd have uint64 since uint32 only allows 8B point IDs)
        - Each vector will take 32bytes (256 bits), so 10B \* 32 bytes = 320 GB
        - So total requirements are like 820 + 80 + 320 GB = 1220 GB for 10B vectors (i.e. 122GB for 1B vectors) which is just 122/8000\*100 = 1.6% of the index (nice!)
      - The index must be kept in RAM for fast queries and also should have be kept on disk to (re)load from.
      - Disk cost: 122 GB \* \$ 0.35/GB [for regional SSD](https://github.com/sirupsen/napkin-math) = 43 \$ / month per 1B docs
      - RAM cost: 122 GB \* 2 \$ / GB / month = 244 \$ / month for 1B docs
      - Total cost: 287 \$ / month for 1B docs
  - Overall, vector search cost for them is like 6.8k + 287 = 7k \$ / month for 1B vectors. Note how this is ~2.5x of lexical search and most of it comes from just storing the original vectors for re-ranking.

## üóÇÔ∏è Metadata storage:

- Each original vector is 8KB, but they also need to store metadata like title, url, publishedDate, author, id, image, favicon, text, summary, etc. I'm assuming it's gonna take another 512bytes (0.5KB) for each doc. Why? because their [search API example](https://docs.exa.ai/reference/search) returns page entries that can be stored in < 300 bytes.
- So total metadata storage on disk is 1B docs \* 0.5KB = 0.5TB for 1B doc metadata. This means 500 GB \* 0.35 \$ / GB = 175 \$ / month for 1B docs.
- Keeping metadata in RAM makes sense and would take, 512 TB \* 2 \$ / GB = 1k \$ / month for 1B docs.

## üì¶ Content storage:

- [Fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) has 26B docs consuming 108 TB. 1B docs => 4.15TB
- [English Wikipedia Text](https://en.wikipedia.org/wiki/Wikipedia:Statistics#Articles) has 7M docs with 58GB => 1B docs in 8.3TB (Text heavy)
- So let's assume exa has curated high quality pages, where 1B pages take 5TB on avg
- S3 cost: 5 \* 1e3 \* 0.02 \$ = 100 \$ / month
- Disk cost: 5 \* 1e3 \* 0.08 \$ (Ephemeral SSD) = 400 \$ / month
- Total is 500 \$ / month for 1B docs (not bad)

## üåêüì§ Network egress costs:

The APIs respond back to the user and that also costs money. [Napkin maths](https://github.com/sirupsen/napkin-math) says it's `$0.1` per GB for "internet egress"

I did a basic search API call with Exa and response size was around 3k bytes ~= 3KB. If they serve, 100M requests / month (only 33 RPS required, with peak 100 RPS provisioned), it would cost them

3 KB \* 100M => 300 GB of egress => 300 \* 0.1 \$ = 30 \$ / month for 100M requests which is very cheap!

## üåçüîÅ Replication:

I queried Exa AI DNS records and looks like they are only present in AWS `us-west-2` region. This has downsides because it increases latency a lot. The round trip time from India is `600-800ms`. You can find ping times from all over the world [here](https://globalping.io/?measurement=g9UVVPjZD7g3OmqV). But yeah they can expand their regions as they get more customers across the globe because each replication is going to be huge jump in their infra cost.

You can also use this command to get rough idea of latency from your machine:

```sh
time curl --head https://api.exa.ai/search
```

## üí∞üìä Overall costs:

| Component              | Cost for 1B docs / month | Cost for 10B docs / year (120x) |
| ---------------------- | ------------------------ | ------------------------------- |
| Content storage        | 500 USD                  | 60k USD                         |
| Metadata storage       | 1.2k USD                 | 144k USD                        |
| Lexical search storage | 2.8k USD                 | 336k USD                        |
| Vector search storage  | 7k USD                   | 840k USD                        |
| Egress (API response)  | 30 USD                   | 3.6k USD                        |
| TOTAL                  | 28k USD                  | 1.38M USD                       |

So the cost of running all this is easily 2M \$ / year since I haven't even considered CPU/GPU cost, crawling, model fine tuning, and rest of the infra.

## Search latency analysis & CPU requirements:

[Exa benchmarks](https://exa.ai/blog/fastest-search-api) say their p50 = 420ms and p95 = 600ms.

Out of this 420ms, we have these components:

- Network latency: They say it's 50ms. So we have 420-50ms = 370ms
- Embedding generation: I'm assuming 70ms with 32 batch size based on [this](https://github.com/huggingface/text-embeddings-inference)
- Re-ranking with cross encoder models: Let's assume another 200ms because it has to rank top 100 items against the query.
- Vector search + Text + Re-ranking with uncompressed vectors. Remaining: 370-70-200=100ms to execute search.

Each of the 10 clusters could return 100 items each and then we re-rank them. This sounds reasonable.

To read 1k original vectors from disk randomly (i.e. 1k \* 8kb = 8MB of data), it would take 100 Œºs \* 1k = 100ms which is horrible for search.

Assuming a cache hit ratio of 90%, you'll only have to read 100 vectors from disk. Then you can achieve:

- Disk: 100 Œºs \* 100 = 10ms
- RAM: 50 ns \* 9.5k = 475 Œºs

And interestingly, they have a "fast search" mode as well, where I think they just skip the re-ranking with original vectors part because I did a few queries and I found the difference to be around 10ms. It's not a coincidence, fundamentals and napkin maths work ;)

So overall vector search might be like:

- Created IVF Index with 10B BQ + MRL truncated vectors that are split in 100k clusters with 100k points each
- Convert query into embedding: 30ms
- Search: 100k centroids comparison + 100k points vector search in 10 clusters in parallel => 10 \* 100 results = 1k results
- Rerank 1: Rerank 1000 vectors by original reading vectors from disk (180ms)
- Rerank 2: Cross encoder models: Probably a cross-encoder model

5M 1536 => 4096, 2M op/sec

let's assume batch size is 32,

100M vectors, 512 bytes/vector (fp16 256), 1TB/memory bandwidth (must have connected multiple devices??)

50 GB to read, 1 TB / sec

1000 / 50 => 20 times / sec. Or each full memory read takes, 50ms

B >= QPS \* full_scan_latency

100 / 20 = 5

Related to https://www.youtube.com/watch?v=jlMAX2Oaht0

. Compute comes at 10 \$ / month

If you assume 10B vectors, 100k clusters, each with 100k vectors. You search in top `0.01%` of these 100k clusters = 10 clusters. So 2nd step has go parallely search in 10 clusters each of size 100k

100k + 1.2 _ 100k = 2.2 _ 100K. Assuming 100K takes 5ms. This should be finished within 12ms

Then you need to read 1M 8KB (8GB) vectors from disk randomly, each takes 100 us => 1M \* 0.1 ms = 100k ms => 100s (clearly wrong)
If same thing was sequential, it would have take 1us for each and hence 8GB could be read in 1s (hmm, this contradicts with numbers in napkin maths throghtput side. why?)

Remaining

Search is more IO bound than compute bound, so I'm assuming out of 500ms, only 250ms is spent in embedding & lexical/vector search and we want only 70% CPU utilization and add 1.5x for peak load safety.

```py
cpu_cores = (RPS * actual_cpu_time_per_req) / (target_cpu_util * 1000ms) * 1.5
# For 100 RPS (10M request / day)
(100 * 50) / (0.7 * 1000) * 1.5  = 10.7 cores # nice
```

Each core costs 10 \$ if you reserve, so cost of CPU should be just 107\$ / month to serve 100 RPS

### What if Exa used HNSW instead of IVF Index?

- I know HNSW returns 10 results within 3ms with 1M vectors with SQ with 97% recall. The avg search time complexity of HNSW is `log(N)` while IVF is `log(sqrt(N))`. In theory, this means to go from 1M to 10B, IVF latency balloons by `math.sqrt(1e10) / math.sqrt(1e6) = 100x` while HNSW search time only increases by `log(1e10) / log(1e6) = 10/6 = 1.6x`. So HNSW should remain within 5-10ms even if implemented correctly while IVF index search would reach 300ms (assuming same 3ms at 1M vectors). This is kinda aligned with what we see in numbers reported by Exa benchmarks as 420ms is p50 and p95 is 600ms.
- Even in [this blog](https://blog.vespa.ai/billion-scale-knn-part-two/) by Vespa, they use 72 vCPUs to hold 1B 100dim vectors that are binary quantized. It does writes at 32k RPS and with 72 cores it means avg write latency is RPS = cores \* 1/latency => latency = cores / RPS = 72 / 32_000 = 2.25ms. While search latency with HNSW with just 4ms with 90% recall.
- The cost of building and storing index would shoot up because of higher RAM and CPU requirements for HNSW. Perplexity uses HNSW and hence was able to claim lower latency in their recent benchmarks against Exa. imo it makes sense for Exa to also do this and so their machines get more time to spend on re-ranking and LLM based filtering (websets) afterwards.
- Something similar was also proven in [my previous benchmark blog with Nirant](https://nirantk.com/writing/pgvector-vs-qdrant). Although, I only see my own dumb mistakes now.
- If HSNW was running at 200B scale (Perplexity), that would be `3 * math.log(200 * 1e9) / math.log(1e6) = 3 * 1.88 ~= 6ms`

### Exa's profitability and margins:

Exa charges 5\$ / 1000 request, to make 2M \$ they need to serve 2M / 5 \* 1000 = 400k requests total (for reference, Google handles 10B requests per day). So per day Exa needs to handle (400k/365) = ~1100 requests per day to break even in terms of infra cost which can be handled with just 100 peak RPS (11 RPS = 1M request / day, 33 RPS = 3M request / day. Assume 3x for peak load)

Each 1000 request costs 5\$
If you're using Exa at 5 QPS,

If you wanna saturate 5QPS, you'll do 1000 reqeusts in 1000 req / 5req/sec = 200s and it will cost 5 \$.

There are 86400 seconds in a day, so one client can at max spend (86400 / 200) \* 5 = 432 \* 5 = 2160 \$ per day.

5 QPS => 450k requests / day (11 RPS = 1M req / day)

If Exa has 10 such clients, they can make 22k per day or .

Exa just raised 85M$\ and at a 700M$ valuation, assuming 60-120x multiple (high but possible in a super hot AI sector, perplexity is at 180x, HF is at 90x, Cohere is at 250x), they might be making 5-12M\$ in ARR

5M ARR means 14k\$ / day

14k\$ / day means, they serve 31 RPS

To serve 31 RPS with p50 of 420ms, you just need 13 cores (todo: a lot of this is just GPU + IO + network time (they say 50ms), so factor that as well)

Exa AI python SDK downloads are at 30k / day. Assume only 0.01% converts to paying user, that means 3 new users daily (is this a fair assumption?)

## Closing remarks:

Google search has been one of the most fascinating products for me since childhood. It started with Pagerank, added lots of heuristics and bunch of ML models over time and now it's beautiful to witness that now even that's getting replaced by generalizable language models. It's literally a [Bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) 101. We live in an exciting time where startups are re-creating Google scale search that's on-par in quality and are preparing for agents to take over the web traffic in near future. We have companies like Exa, Parallel, and Perplexity are effectively "caching" the web and serving "reads" in an optimized way. While other agents (like Perplexity Comet, OpenAI Operator, and others) want to "write" to the web (i.e. take actions). Web & Search are changing and I'm very excited about their futures!

In search you have to pick two between cost, quality, and speed. Exa.ai seems to optimize for quality & speed which is great for the use cases they want to serve! They also offer Websets that go deeper into the web and use LLMs to filter out documents, it optimizes mainly for quality because speed and cost and expected to to be high.

Thanks to my friend [Nirant Kasliwal](https://in.linkedin.com/in/nirant) for reviewing the blog.

### References:

- https://exa.ai/blog/building-web-scale-vector-db
- https://exa.ai/blog/bm25-optimization ->
- https://exa.ai/blog/scaling-our-highlights-server -> TODO
- https://exa.ai/blog/evals-at-exa -> TODO
- https://www.perplexity.ai/search/what-s-the-cost-of-running-per-9IfuBvj2RnCPwwoTlMGyoA#0
