---
title: 'Search Internals: Exa AI napkin maths'
date: '2025-09-16'
lastmod: '2025-09-16'
tags: ['search', 'internals', 'infra']
draft: true
summary: ''
images: ['/static/images/blogs/exa.png']
authors: ['default']
---

[Exa](https://exa.ai/) is a cool search engine company. They recently [raised 85M\$ for Series B](https://exa.ai/blog/announcing-series-b) and attracted competition from bigger players because of their fast growth. There are multiple parts of Exa AI architecture that they have described in [their blogs](https://exa.ai/blog). I've compiled them together and added some of my own notes and lots of assumptions for infra cost [napkin maths](https://www.youtube.com/watch?v=IxkSlnrRFqc) as a fun exercise for myself. Please take my estimates with a grain of salt and let me know if I made any wrong/suboptimal assumptions :D

### Lexical search:

- BM25 index (i.e. postings lists) costed them 1.8TB RAM (1.8 \* 10^12 bytes) for 1B (10^9) billion docs initially.
  - After optimizations in [this post](https://exa.ai/blog/bm25-optimization) it dropped to 1.2TB (-50%) RAM per 1B docs.
  - Assuming it's all kept in memory and relying on [napkin maths](https://github.com/sirupsen/napkin-math), this means 2 \$ / month / GB \* 1200 GB = 2400 \$ / month
  - There must be a version of it on disk as well, so 1200 GB \* 0.35 \$ = 420 \$ / month
  - So total lexical search cost might be around 3000 \$ / month for 1B docs (assuming more things like stopwords and extra info required to run the search)

### Vector search:

- Trained custom [Matryoshka embeddings](https://huggingface.co/blog/matryoshka) with 4096 dims but only used 256 dims in memory (i.e. MRL truncation).
- I think they might be having a fine-tuned version of [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B) which fits the description well.
- [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B) has a massive context length of 32K tokens. As per this quick [research](https://www.perplexity.ai/search/average-words-per-page-on-the-qjaJmyDVQ823yG.lh.Ba3g#0), most internet pages and even research papers would fit in this context length. So chunking isn't required. 1 document = 1 vector.
- Used [binary quantization](https://qdrant.tech/articles/binary-quantization/) (BQ)
- Precomputed possible outputs of BQ computations in CPU registers (instead of RAM) for similarity computation. Very cool trick!
- Re-ranking with original vectors to compensate for all this approximation.
- Each uncompressed vector would take 4096 dims \* 2 bytes (assuming fp16) = 8KB by default. This means 1B vectors would take 8 \* 10^3 bytes \* 10^9 = 8 \* 10^12 = 8 TB of disk storage for 1B original vectors.
  - By truncating matryoshka it becomes, 8 / (4096 / 256) TB = 8 / 16 TB = 512 GB for 1B vectors.
  - With binary quantization, 16 bits (since we assumed fp16) becomes 1 bit. So 512 GB / 16 = 32 GB per 1B vectors (insane 16\*16=256x reduction)
  - They use something that's like [IVF Index](https://youtu.be/chz74Mtd1AA?si=DvvDE3rO9HvVvox7&t=429) (popularly used in FAISS & pgvector). It basically does k-means clustering and during search time, finds the relevant clusters by comparing distance with respective centroids, and only queries few of those clusters.
  - I think they build multiple vector indices. Their vector search might be having:
    - Original vectors:
      - If all kept on disk: 8 \* 1e3 \* 0.35 \$ = 2800 \$ / month for 1B docs
      - If all kept in memory: 8 \* 1e3 \* 2 \$ = 16000 \$ / month for 1B docs
      - The must be caching it so actual RAM usage and cost would be in-between. I'd expect something like 4k \$ / month for 1B docs consuming 2TB RAM
    - Vector index with MRL truncation + re-ranking with original vectors for their "neural search"
      - Exa uses IVF Index for search, it needs centroids along with original vectors, assuming they have 10B vectors and they have 100K clusters, each having 100K vectors.
        - Each cluster will have a centroid which require 100K \* 4096 \* 2 bytes = 820GB for 10B vectors (or 82GB for 1B vectors)
        - Each vector ID needs to stored in the IVF Index, 10B \* 8 bytes = 80 GB (assuming that for point id, they'd have uint64 since uint32 only allows 8B)
        - So total size requirements are like 820GB + 80GB = 900 GB for 10B vectors (or 90GB for 1B vectors) which is 90/8000\*100 = 1.2% of the index (nice!)
      - Index disk storage cost: 90 \* \$ 0.35/GB [for regional SSD](https://github.com/sirupsen/napkin-math) = 32 \$ / month per 1B docs
      - All this must be kept in RAM for lowest latency, so it would cost them 90 GB \* 2 \$ / GB / month = 180 \$ / month for 1B docs
    - Vector index with BQ + MRL truncation + re-ranking with original vectors for their "neural search" (todo: rethink if there's a better combination)
      - Index Storage cost: 32 \$ / 16 (16 bits => 1 bit) = 2 \$ / month / 1B vectors (damn, BQ cost savings are insane. this is kinda funny)
      - RAM Cost: 180 / 16 = 11.25 \$ / month
    - They have a "fast search" mode as well, where I think they just skip the re-ranking with original vectors part. I did some experiments and I didn't find more than 10ms difference between this and neural search. I need to find what's this might be. This step must be very small which can happen only if they are
  - Overall, vector search cost for them is like 4k + (32+180) + (2+11.25) = 4.2k \$ / month for 1B vectors. Note how this is ~1.5x of lexical search and most of it comes from just storing the original vectors for re-ranking. It would be interesting to see what happens if they get rid of original vectors and replace them with just cross encoders.

## Metadata storage:

- Each original vector is 8KB, but they also need to store metadata like title, url, publishedDate, author, id, image, favicon, text, summary, etc. I'm assuming it's gonna take another 512bytes (0.5KB) for each doc. Why? because their [search API example](https://docs.exa.ai/reference/search) returns page that can be stored in < 300 bytes without compressing but let's ignore that to keep search fast.
- So total metadata storage on disk is 1B docs \* 0.5KB = 0.5TB for 1B doc metadata. This means 500 GB \* 0.35 \$ / GB = 175 \$ / month for 1B docs.
- Keeping metadata in RAM makes sense and would take, 512 TB \* 2 \$ / GB = 1k \$ / month for 1B docs.

### Content storage:

- [Fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) has 26B docs consuming 108 TB. 1B docs => 4.15TB
- [Wikipedia](https://www.perplexity.ai/search/number-of-pages-and-size-of-wi-Y8GkQm8hQlKfgm8YcAJxAA#0) has 64M docs with 156GB => 1B docs in 2.4TB (todo: cross check again)
- [English Wikipedia Text](https://en.wikipedia.org/wiki/Wikipedia:Statistics#Articles) has 7M docs with 58GB => 1B docs in 8.3TB
- So let's assume exa has total of 10B docs and 1B takes 5TB on avg. So total is 50TB of s3 storage
- S3 cost: 50 \* 1e3 \* 0.02 \$ = 1000 \$ / month
- Disk cost: 50 \* 1e3 \* 0.08 \$ (Ephemeral SSD) = 4k \$ / month
- Total is 5k \$ / month for 1B docs

### Network egress costs:

The APIs respond back to the user and will cause significant costs. Let's estimate that.

[Napkin maths](https://github.com/sirupsen/napkin-math) says it's `$0.1` per GB for "internet egress"

I did a basic search and consumed around 3k bytes ~= 3KB. If they serve, 100M requests / month (only 33 RPS required, with peak 100 RPS), it would cost them

3 KB _ 100M => 300 GB of egress => 300 _ 0.1 = 30 \$ / month for 100M requests which is very cheap!

### Replication:

```sh
for ip in $(dig +short api.exa.ai); do
  curl ipinfo.io/$ip
done

# {
#   "ip": "34.216.236.112",
#   "hostname": "ec2-34-216-236-112.us-west-2.compute.amazonaws.com",
#   "city": "Boardman",
#   "region": "Oregon",
#   "country": "US",
#   "loc": "45.8399,-119.7006",
#   "org": "AS16509 Amazon.com, Inc.",
#   "postal": "97818",
#   "timezone": "America/Los_Angeles",
#   "readme": "https://ipinfo.io/missingauth"
# }{
#   "ip": "44.225.245.181",
#   "hostname": "ec2-44-225-245-181.us-west-2.compute.amazonaws.com",
#   "city": "Boardman",
#   "region": "Oregon",
#   "country": "US",
#   "loc": "45.8399,-119.7006",
#   "org": "AS16509 Amazon.com, Inc.",
#   "postal": "97818",
#   "timezone": "America/Los_Angeles",
#   "readme": "https://ipinfo.io/missingauth"
# }{
#   "ip": "52.36.132.24",
#   "hostname": "ec2-52-36-132-24.us-west-2.compute.amazonaws.com",
#   "city": "Boardman",
#   "region": "Oregon",
#   "country": "US",
#   "loc": "45.8399,-119.7006",
#   "org": "AS16509 Amazon.com, Inc.",
#   "postal": "97818",
#   "timezone": "America/Los_Angeles",
#   "readme": "https://ipinfo.io/missingauth"
# }
```

## Overall costs:

| Component              | Cost for 1B docs / month | Cost for 10B docs / year |
| ---------------------- | ------------------------ | ------------------------ |
| Content storage        | 5k USD                   | 600k USD                 |
| Metadata storage       | 1.2k USD                 | 144k USD                 |
| Vector search storage  | 4.2k USD                 | 504k USD                 |
| Lexical search storage | 3k USD                   | 360k USD                 |
| Egress (API response)  | 30 USD                   | 120 USD                  |
| TOTAL                  | 28k USD                  | 1.6M USD                 |

Damn! The cost of running all this is easily 1M \$ since I haven't even considered CPU/GPU cost, replication for throughput, and rest of the infra. Exa charges 5\$ / 1000 request, to make 3M \$ they need to serve 3M / 5 \* 1000 = 600k requests (for reference, Google handles 10B requests per day). So per day Exa needs to handle (600k/365) = ~2000 requests per day to break even in terms of infra cost which can be handled with just 100 peak RPS (11 RPS = 1M request / day, 33 RPS = 3M request / day. Assume 3x for peak load)

## Search latency analysis & CPU requirements:

[Exa benchmarks](https://exa.ai/blog/fastest-search-api) say their p50 = 420ms and p95 = 600ms.

Out of this 420ms, we have 3 types of steps:

- Embedding generation
- Vector search + Text + Re-ranking with uncompressed vectors
- Re-ranking with cross encoder models

let's assume batch size is 32,

. Compute comes at 10 \$ / month

If you assume 10B vectors, 100k clusters, each with 100k vectors. You search in top `0.01%` of these 100k clusters = 10 clusters. So 2nd step has go parallely search in 10 clusters each of size 100k

100k + 1.2 _ 100k = 2.2 _ 100K. Assuming 100K takes 5ms. This should be finished within 12ms

Then you need to read 1M 8KB (8GB) vectors from disk randomly, each takes 100 us => 1M \* 0.1 ms = 100k ms => 100s (clearly wrong)
If same thing was sequential, it would have take 1us for each and hence 8GB could be read in 1s (hmm, this contradicts with numbers in napkin maths throghtput side. why?)

Remaining

Search is more IO bound than compute bound, so I'm assuming out of 500ms, only 250ms is spent in embedding & lexical/vector search and we want only 70% CPU utilization and add 1.5x for peak load safety.

```py
cpu_cores = (RPS * actual_cpu_time_per_req) / (target_cpu_util * 1000ms) * 1.5
# For 100 RPS (10M request / day)
(100 * 50) / (0.7 * 1000) * 1.5  = 10.7 cores # nice
```

Each core costs 10 \$ if you reserve, so cost of CPU should be just 107\$ / month to serve 100 RPS

### What if Exa used HNSW instead of IVF Index? (TODO: Clean up and back with numbers)

- I know HNSW returns 10 results within 3ms with 1M vectors with SQ with 97% recall. The search time complexity of HNSW is `log(N)` while IVF is `log(sqrt(N))`. This means to go from 1M to 10B, IVF grows by `math.sqrt(1e10) / math.sqrt(1e6) = 100x` while HNSW search time grows only by `log(1e10) / log(1e6) = 10/6 = 1.6`. So it should remain within 5ms (Let's take 25ms assuming other parts don't scale equally well) if everything scales while IVF index search could reach 500ms (which is similar to what we see in numbers reported by Exa benchmarks as 420ms is p50).
- However, the cost of building and storing index will shoot up which makes it tough for Exa since they have regular updates from the web.
- Something similar is also proven in [this blog by Vespa](https://blog.vespa.ai/billion-scale-knn-part-two/)
- If HSNW was running at 200B scale (Perplexity), that would be `3 * math.log(200 * 1e9) / math.log(1e6) = 3 * 1.88 ~= 6ms`

## Conclusion:

In search you have to pick two between cost, quality, and speed. Exa.ai seems to optimize for quality & speed which is great for the use cases they want to serve! They also offer Websets that go deeper into the web and use LLMs to filter out documents, it optimizes mainly for quality because speed and cost and expected to to be high.

Thanks to my friend [Nirant Kasliwal](https://in.linkedin.com/in/nirant) for reviewing the blog.

### References:

- https://exa.ai/blog/building-web-scale-vector-db
- https://exa.ai/blog/bm25-optimization ->
- https://exa.ai/blog/scaling-our-highlights-server -> TODO
- https://exa.ai/blog/evals-at-exa -> TODO
- https://www.perplexity.ai/search/what-s-the-cost-of-running-per-9IfuBvj2RnCPwwoTlMGyoA#0
