---
title: 'Exa: Napkin Maths Behind the Search Engine for AI'
date: '2025-09-16'
lastmod: '2025-10-02'
tags: ['search', 'internals', 'infra']
draft: true
summary: ''
images: ['/static/images/blogs/exa.png']
authors: ['default']
---

[Exa](https://exa.ai/) is a cool search engine company. They recently [raised 85M\$ for Series B](https://exa.ai/blog/announcing-series-b). There are multiple parts of Exa AI architecture that they have described in their [blogs](https://exa.ai/blog). I've compiled them together and added some of my own notes and **lots of assumptions** for infra cost [napkin maths](https://www.youtube.com/watch?v=IxkSlnrRFqc) as a fun exercise for myself. Please take my estimates with a grain of salt and let me know if I made any wrong or suboptimal assumptions :D

## üîç Lexical/Keyword search (BM25):

- They use the standard [BM25 algorithm](https://www.youtube.com/watch?v=ziiF1eFM3_4&t=698s) for lexical search.
- Initially, this required about 1.8 TB RAM (posting lists + frequency stats) for 1B docs
- After optimizations in [this post](https://exa.ai/blog/bm25-optimization) it dropped to 1.2TB (-50%) RAM for 1B docs

### Napkin maths

- I'm assuming the index is also kept in RAM for faster querying and I'm using rates from [this](https://github.com/sirupsen/napkin-math)
- RAM Cost: 1200 GB √ó \$ 2 / GB = $2400 / month
- Disk Cost: 1200 GB √ó \$ 0.35 / GB = $420 / month
- Total ~= 2800 \$ / month for 1B docs

## üß≠ Vector search:

- They train [Matryoshka embeddings](https://huggingface.co/blog/matryoshka) with 4096 dims but only keep 256 dims in RAM (i.e. MRL truncation)
- They use [binary quantization](https://qdrant.tech/articles/binary-quantization) (BQ) and some clever CPU tricks for vector distance calculations ([SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) + precomputing possible outputs and keeping them in CPU registers for skipping calculation).
- They use [IVF Index](https://youtu.be/chz74Mtd1AA?si=DvvDE3rO9HvVvox7&t=429) (also used in FAISS & pgvector) which an Approximate Nearest Neighbours Algorithm. It basically does k-means clustering, and during search time, finds the relevant clusters by comparing distance with respective cluster centroids, and only queries few of those clusters.
- Lastly, they do re-ranking with original vectors to compensate for all this approximation/compression.

### Napkin maths

- I think their model is a fine-tuned version of [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B) since it fits the description well and it doesn't make sense for such a young startup to start training embeddings from scratch.
- Qwen3-Embedding-8B has a massive context length of 32K tokens. As per this quick [research](https://www.perplexity.ai/search/average-words-per-page-on-the-qjaJmyDVQ823yG.lh.Ba3g#0), most internet pages and even research papers would fit in this context length. So chunking isn't required. 1 document = 1 vector.
- Each uncompressed vector would take 4096 dims \* 2 bytes (assuming fp16) = 8KB by default. This means 1B vectors would take 8 \* 10^3 bytes \* 10^9 = 8 \* 10^12 = 8 TB for 1B docs.
  - By truncating matryoshka it becomes, 8 / (4096 / 256) TB = 8 / 16 TB = 512 GB for 1B vectors.
  - With binary quantization, 16 bits (since fp16) becomes 1 bit. So 512 GB / 16 = 32 GB per 1B vectors (i.e. 16\*16=256x reduction)
  - Their vector search might have these components:
    - Original vectors:
      - Disk cost: 8 \* 1e3 \* 0.35 \$ = 2.8k \$ / month for 1B docs
      - Memory cost: 8 \* 1e3 \* 2 \$ = 16k \$ / month for 1B docs. But this sounds too costly and Pareto principle says 20% of docs will account for 80% queries so let's cache only 25% in RAM and hence it should consume 2TB RAM and cost 4k \$ / month for 1B docs.
      - Total: 2.8 + 4k \$ = 6.8k \$
    - IVF index with BQ + MRL truncation + re-ranking with original vectors for their "neural search"
      - IVF Index needs centroids along with original vectors, I'm assuming they have 10B vectors and they have 100K clusters, each having 100K vectors.
        - Each cluster will have a centroid and I'm assuming they keep uncompressed centroid vectors for maximizing recall.
        - Centroids will require 100K \* 4096 \* 2 bytes = 820GB for 10B vectors (or 82GB for 1B vectors)
        - Each vector ID needs to stored in the IVF Index, 10B \* 8 bytes = 80 GB (assuming that for point id they'd have uint64 since uint32 only allows 8B point IDs)
        - Each vector will take 32bytes (256 bits), so 10B \* 32 bytes = 320 GB
        - So total requirements are like 820 + 80 + 320 GB = 1220 GB for 10B vectors (i.e. 122GB for 1B vectors) which is just 122/8000\*100 = 1.6% of the index (nice!)
      - The index must be kept in RAM for fast queries and also should have be kept on disk to (re)load from.
      - Disk cost: 122 GB \* \$ 0.35/GB [for regional SSD](https://github.com/sirupsen/napkin-math) = 43 \$ / month per 1B docs
      - RAM cost: 122 GB \* 2 \$ / GB / month = 244 \$ / month for 1B docs
      - Total cost: 287 \$ / month for 1B docs
  - Overall, vector search cost for them should be around 6.8k + 287 = 7k \$ / month for 1B vectors. Note how this is ~2.5x of lexical search and most of it comes from just storing the original vectors for re-ranking.

## üóÇÔ∏è Metadata storage:

- For each web page, they also need to store metadata like title, url, publishedDate, author, id, image, favicon, text, summary, etc. I'm assuming it's gonna take another 512bytes (0.5KB) for each doc. Why? because their [search API example](https://docs.exa.ai/reference/search) returns page attributes that can be stored in < 300 bytes.
- So total metadata storage on disk is 1B docs \* 0.5KB = 0.5TB for 1B doc metadata. This means 500 GB \* 0.35 \$ / GB = 175 \$ / month for 1B docs.
- Keeping metadata in RAM makes sense and would take, 512 TB \* 2 \$ / GB = 1k \$ / month for 1B docs.

## üì¶ Content storage:

- [Fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) has 26B docs consuming 108 TB. 1B docs => 4.15TB
- [English Wikipedia Text](https://en.wikipedia.org/wiki/Wikipedia:Statistics#Articles) has 7M docs with 58GB => 1B docs in 8.3TB (Text heavy)
- So let's assume tha Exa crawlers curate pages where 1B pages take 5TB on avg
- S3 cost: 5 \* 1e3 \* 0.02 \$ = 100 \$ / month
- Disk cost: 5 \* 1e3 \* 0.08 \$ (Ephemeral SSD) = 400 \$ / month
- Total is 500 \$ / month for 1B docs (not bad)

## üåêüì§ Network egress costs:

The APIs respond back to the user and that also costs money. [Napkin maths](https://github.com/sirupsen/napkin-math) says it's `$0.1` per GB for "internet egress"

I did a basic search API call with Exa and response size was around 3k bytes ~= 3KB. If they serve, 100M requests / month (only 33 RPS required, with peak 100 RPS provisioned), it would cost them

3 KB \* 100M => 300 GB of egress => 300 \* 0.1 \$ = 30 \$ / month for 100M requests which is very cheap!

## üåçüîÅ Global replication:

I queried Exa AI DNS records and looks like they are only present in AWS `us-west-2` region. This has downsides because it increases latency a lot. The round trip time from India is `600-800ms`. You can find ping times from all over the world [here](https://globalping.io/?measurement=g9UVVPjZD7g3OmqV). But yeah they can expand their regions as they get more customers across the globe because each replication is going to be huge jump in their infra cost.

You can also use this command to get rough idea of latency from your machine:

```sh
time curl --head https://api.exa.ai/search
```

## üí∞üìä Overall costs:

| Component              | Cost for 1B docs / month | Cost for 10B docs / year (120x) |
| ---------------------- | ------------------------ | ------------------------------- |
| Content storage        | 500 USD                  | 60k USD                         |
| Metadata storage       | 1.2k USD                 | 144k USD                        |
| Lexical search storage | 2.8k USD                 | 336k USD                        |
| Vector search storage  | 7k USD                   | 840k USD                        |
| Egress (API response)  | 30 USD                   | 3.6k USD                        |
| TOTAL                  | 28k USD                  | 1.38M USD                       |

So the cost of running all this is easily 2M \$ / year for 10B docs since I haven't even considered CPU/GPU cost, crawling, model fine tuning, and rest of the infra.

## Search latency analysis:

[Exa benchmarks](https://exa.ai/blog/fastest-search-api) say their p50 = 420ms and p95 = 600ms.

Out of this 420ms, they probably have these components:

![Exa Search Pipeline](/static/images/blogs/exa-search-pipeline.png)

### Neural search latency

- Network latency: They say it's 50ms. So we have 420-50ms = 370ms remaining.
- Embedding generation: I'm assuming 70ms with 32 batch size based on [this](https://github.com/huggingface/text-embeddings-inference)
- Re-ranking with cross encoder models: Let's assume another 200ms because it has to rank top 200 (100 from each of lexical and vector search) items against the query.
- Vector search + Re-ranking with uncompressed vectors. Remaining: 370-70-200=100ms.

After the vector search step, each of the 10 clusters could return 100 items each and then we re-rank them with original vectors. It sounds reasonable. But to read 1k original vectors from disk randomly (i.e. 1k \* 8kb = 8MB of data), it would take 100 Œºs \* 1k = 100ms which is horrible.

However, assuming a cache hit ratio of 90%, you'll only have to read 100 vectors from disk. Then it takes:

- Disk: 100 Œºs \* 100 = 10ms
- RAM: 50 ns \* 900 = 47 Œºs

And assuming a cache hit ratio of 70%, you'll only have to read 300 vectors from disk. Then it takes:

- Disk: 100 Œºs \* 300 = 30ms
- RAM: 50 ns \* 700 = 35 Œºs

And interestingly, they have a "fast search" mode as well, where I think they just skip the re-ranking with original vectors part because I did a few queries and I found the difference to be around 10-40ms. It's probably not a coincidence :D

This means we have at least 60-90ms for the vector search part. Which sounds doable since we only have to query 1 centroid cluster + 10 clusters of 100k BQ vectors each which can be done in parallel within the specified duration. And I'd argue that you might be able to do more things like putting a small language model to rewrite the query and guess the "category" of the query or something like RRF (reciprocal ranking fusion)

### Keyword search latency

The [Exa blog](https://exa.ai/blog/bm25-optimization) clearly says they get sub 500ms latency for retrieving top 1000 docs. Even if they retrieve top 10 or 100, it would just slightly faster because you need to rank the same documents.

However it's a known fact that BM25 latency degrades faster than vector search with

- Longer queries (More posting lists to scan)
- More data (especially if you have common term will have more docs to compare)

For example effect of longer queries part can be seen in [this benchmark](https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-4/#:~:text=Benchmarking%20Results):

| Retriever      | Average latency (ms) | p999 latency (ms) |
| -------------- | -------------------- | ----------------- |
| Keyword (WAND) | 16                   | 209               |
| Vector (HNSW)  | 18                   | 28                |

You can see how avg latency is similar for both vector search keyword search but p999 is 7.5x more for keyword search. (Yes, Exa uses IVF Index not HNSW so it would also degrade faster with scale but I think BM25 would degrade more)

Also, since Exa operates with atleast 10B docs (assumed earlier), I can imagine that they split 10B docs into multiple "BM25 shards" and query them in parallel and combine results afterwards.

## What if Exa used HNSW instead of IVF Index?

- In this [blog](https://blog.vespa.ai/billion-scale-knn-part-two/), the experiment shows a machine with 72 vCPUs dealing with 1B 100dim vectors that are binary quantized. It shows search latency of HNSW with recall 50% within 2ms and 4ms with 90% recall. It also handles realtime indexing at 15.4k RPS and with 72 cores it means avg write latency is RPS = cores \* 1/latency => latency = cores / RPS = 72 / 32_000 = 4.68ms (just a little higher than search because hnsw mainly needs to search and then it knows where to add new links)
- Also the avg search time complexity of HNSW is `log(N)` while IVF is `log(sqrt(N))`. **In theory**, it means that to go from "shards" of 1M to 1B docs, IVF latency balloons by `math.sqrt(1e9) / math.sqrt(1e6) = 33x` while HNSW search time only increases by `log(1e10) / log(1e6) = 10/6 = 1.5x`. So even if you asssume they perform similar at 1M scale - say 2ms with single thread. HNSW should remain within 5ms even while IVF index search would take 70-100ms. That's why Exa would have to split into 10B docs into smaller 100k clusters and query only 10 of them them in parallel and that will degrade recall.
- To add, in IVF, the cluster sizes vary and hence like BM25, the query latencies (avg latency vs p999) would vary more depending on the query.
- Something similar was also proven in [my previous benchmark blog](https://nirantk.com/writing/pgvector-vs-qdrant) with Nirant that compared Qdrant's HNSW and PGVector's IVF Index and it led to implementation of HNSW in pgvector.
- The cost of building and storing index would rise because of higher RAM and CPU requirements if Exa switched to HNSW. But Perplexity uses HNSW and hence was able to claim lower latency against Exa in their [recent benchmarks](https://www.perplexity.ai/api-platform/resources/architecting-and-evaluating-an-ai-first-search-api#:~:text=community%20as%20well.-,Search%20API%20Latency,-(milliseconds%3B%20requests%20initiated). In my opinion it makes sense for Exa to also do this and so they get more time to spend on re-ranking and LLM based filtering (websets) afterwards.

## Closing remarks:

Web search started in 1994 with [Yahoo directories](https://en.wikipedia.org/wiki/Yahoo_Directory), then search engines like Alta Vista introduced Keyword search using [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) in late 1990s, and then eventually Google came in 1998 and dominated the game with [Pagerank](https://en.wikipedia.org/wiki/PageRank). Over the next two decades, Google added lots of heuristics and statistics based approaches. Then around 2015 they started integrating ML models such as RankBrain and BERT, enabling semantic understanding and context-aware ranking.

And very recently, we see that even those are being [surpassed](https://exa.ai/blog/evals-at-exa) by more generalizable LLMs. Infact, now you can ask more [complex queries](https://exa.ai/blog/websets-evals) to the web which were impossible with previous techniques. It's literally [Bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) 101 which says compute-driven solutions beat human designed heuristics.

We‚Äôre entering an era where startups are recreating Google scale search and number of searches will rise exponentially because of AI agents driving majority of search request. That's why products like Exa, Parallel, and Perplexity are effectively "caching" the web and serving "reads" in an optimized way. While others like Perplexity Comet, OpenAI Operator, etc want to "write" to the web (i.e. take actions). Web & Search are changing and I'm very excited about their futures!

## Acknowledgements:

Thanks to my friend [Nirant](https://in.linkedin.com/in/nirant) for reviewing the blog.

- https://github.com/sirupsen/napkin-math
- https://exa.ai/blog/building-web-scale-vector-db
- https://exa.ai/blog/bm25-optimization
- https://exa.ai/blog/fastest-search-api
- https://exa.ai/blog/evals-at-exa
- https://exa.ai/blog/websets-evals
