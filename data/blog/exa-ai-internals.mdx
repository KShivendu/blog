---
title: 'Search Internals: Exa AI napkin maths'
date: '2025-09-16'
lastmod: '2025-09-16'
tags: ['search', 'internals', 'infra']
draft: true
summary: ''
images: ['/static/images/blogs/exa.png']
authors: ['default']
---

[Exa](https://exa.ai/) is a cool search engine company. They recently [raised 85M\$ for Series B](https://exa.ai/blog/announcing-series-b). There are multiple parts of Exa AI architecture that they have described in their [blogs](https://exa.ai/blog). I've compiled them together and added some of my own notes and **lots of assumptions** for infra cost [napkin maths](https://www.youtube.com/watch?v=IxkSlnrRFqc) as a fun exercise for myself. Please take my estimates with a grain of salt and let me know if I made any wrong or suboptimal assumptions :D

### Lexical search:

- BM25 index (i.e. postings lists + freq stats) costed them 1.8TB RAM for 1B billion docs initially.
  - After optimizations in [this post](https://exa.ai/blog/bm25-optimization) it dropped to 1.2TB (-50%) RAM per 1B docs.
  - Assuming it's all kept in memory and relying on [napkin maths](https://github.com/sirupsen/napkin-math), this means 2 \$ / month / GB \* 1200 GB = 2400 \$ / month
  - There must be a version of it on disk as well, so 1200 GB \* 0.35 \$ = 420 \$ / month
  - So total lexical search cost might be around 3000 \$ / month for 1B docs

### Vector search:

- Trained custom [Matryoshka embeddings](https://huggingface.co/blog/matryoshka) with 4096 dims but only used 256 dims in memory (i.e. MRL truncation).
- They use [binary quantization](https://qdrant.tech/articles/binary-quantization/) (BQ). Precomputed possible outputs of BQ computations in CPU registers (instead of RAM) for similarity computation. Very cool trick!
- They do re-ranking with original vectors to compensate for all this approximation.
- I think they might be having a fine-tuned version of [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B) which fits the description well. It has a massive context length of 32K tokens. As per this quick [research](https://www.perplexity.ai/search/average-words-per-page-on-the-qjaJmyDVQ823yG.lh.Ba3g#0), most internet pages and even research papers would fit in this context length. So chunking isn't required. 1 document = 1 vector.
- Each uncompressed vector would take 4096 dims \* 2 bytes (assuming fp16) = 8KB by default. This means 1B vectors would take 8 \* 10^3 bytes \* 10^9 = 8 \* 10^12 = 8 TB of disk storage for 1B original vectors.
  - By truncating matryoshka it becomes, 8 / (4096 / 256) TB = 8 / 16 TB = 512 GB for 1B vectors.
  - With binary quantization, 16 bits (since fp16) becomes 1 bit. So 512 GB / 16 = 32 GB per 1B vectors (i.e. 16\*16=256x reduction)
  - They use [IVF Index](https://youtu.be/chz74Mtd1AA?si=DvvDE3rO9HvVvox7&t=429) (popularly used in FAISS & pgvector). It basically does k-means clustering, and during search time, finds the relevant clusters by comparing distance with respective centroids, and only queries few of those clusters.
  - I think they build multiple vector indices. Their vector search might be having:
    - Original vectors:
      - To keep vectors on disk: 8 \* 1e3 \* 0.35 \$ = 2800 \$ / month for 1B docs
      - To keep vectors in memory: 8 \* 1e3 \* 2 \$ = 16000 \$ / month for 1B docs. But that's too costly and it's better to cache, so I'd expect something like 25% in RAM and hence it should consume 2TB RAM and cost 4k \$ / month for 1B docs
    - Vector index with MRL truncation + re-ranking with original vectors for their "neural search"
      - Exa uses IVF Index for search, it needs centroids along with original vectors, assuming they have 10B vectors and they have 100K clusters, each having 100K vectors.
        - Each cluster will have a centroid and I'm assuming they keep original vectors for centroid for max recall
        - It will require 100K \* 4096 \* 2 bytes = 820GB for 10B vectors (or 82GB for 1B vectors)
        - Each vector ID needs to stored in the IVF Index, 10B \* 8 bytes = 80 GB (assuming that for point id they'd have uint64 since uint32 only allows 8B point IDs)
        - So total size requirements are like 820GB + 80GB = 900 GB for 10B vectors (or 90GB for 1B vectors) which is 90/8000\*100 = 1.2% of the index (nice!)
      - Index disk storage cost: 90 \* \$ 0.35/GB [for regional SSD](https://github.com/sirupsen/napkin-math) = 32 \$ / month per 1B docs
      - All this must be kept in RAM for lowest latency, so it would cost them 90 GB \* 2 \$ / GB / month = 180 \$ / month for 1B docs
    - Vector index with BQ + MRL truncation + re-ranking with original vectors for their "neural search"
      - Index Storage cost: 32 \$ / 16 (16 bits => 1 bit) = 2 \$ / month / 1B vectors (damn, BQ cost savings are insane. this is kinda funny)
      - RAM Cost: 180 / 16 = 11.25 \$ / month
    - They have a "fast search" mode as well, where I think they just skip the re-ranking with original vectors part. I did some experiments and I didn't find more than 10ms difference between this and neural search. I need to find what's this might be. This step must be very small which can happen only if they are
  - Overall, vector search cost for them is like 2.8k + 4k + (32+180) + (2+11.25) = 7k \$ / month for 1B vectors. Note how this is ~2.3x of lexical search and most of it comes from just storing the original vectors for re-ranking.

## Metadata storage:

- Each original vector is 8KB, but they also need to store metadata like title, url, publishedDate, author, id, image, favicon, text, summary, etc. I'm assuming it's gonna take another 512bytes (0.5KB) for each doc. Why? because their [search API example](https://docs.exa.ai/reference/search) returns page entries that can be stored in < 300 bytes.
- So total metadata storage on disk is 1B docs \* 0.5KB = 0.5TB for 1B doc metadata. This means 500 GB \* 0.35 \$ / GB = 175 \$ / month for 1B docs.
- Keeping metadata in RAM makes sense and would take, 512 TB \* 2 \$ / GB = 1k \$ / month for 1B docs.

### Content storage:

- [Fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) has 26B docs consuming 108 TB. 1B docs => 4.15TB
- [English Wikipedia Text](https://en.wikipedia.org/wiki/Wikipedia:Statistics#Articles) has 7M docs with 58GB => 1B docs in 8.3TB (Text heavy)
- So let's assume exa has curated high quality pages, where 1B pages take 5TB on avg
- S3 cost: 5 \* 1e3 \* 0.02 \$ = 100 \$ / month
- Disk cost: 5 \* 1e3 \* 0.08 \$ (Ephemeral SSD) = 400 \$ / month
- Total is 500 \$ / month for 1B docs (nice!)

### Network egress costs:

The APIs respond back to the user and that also costs money. [Napkin maths](https://github.com/sirupsen/napkin-math) says it's `$0.1` per GB for "internet egress"

I did a basic search API call with Exa and response size was around 3k bytes ~= 3KB. If they serve, 100M requests / month (only 33 RPS required, with peak 100 RPS), it would cost them

3 KB \* 100M => 300 GB of egress => 300 \* 0.1 \$ = 30 \$ / month for 100M requests which is very cheap!

### Replication:

I tried finding how many replicas does Exa AI have at the moment and looks like they have only 1 replica right now. And that's in AWS `us-west-2` region.

This has downsides because it increases latency a lot. The round trip time from India is `600-800ms`. You can find ping times from all over the world [here](https://globalping.io/?measurement=g9UVVPjZD7g3OmqV). But yeah they can expand their regions as they get more users across the globe because each replication is going to be huge jump in their infra cost.

You can use this script to find the details:

```sh
for ip in $(dig +short api.exa.ai); do
  curl ipinfo.io/$ip
done
```

## Overall costs:

| Component              | Cost for 1B docs / month | Cost for 10B docs / year |
| ---------------------- | ------------------------ | ------------------------ |
| Content storage        | 500 USD                  | 60k USD                  |
| Metadata storage       | 1.2k USD                 | 144k USD                 |
| Vector search storage  | 7k USD                   | 840k USD                 |
| Lexical search storage | 3k USD                   | 360k USD                 |
| Egress (API response)  | 30 USD                   | 3.6k USD                 |
| TOTAL                  | 28k USD                  | 1.4M USD                 |

So the cost of running all this is easily 2M \$ since I haven't even considered CPU/GPU cost and rest of the infra. Exa charges 5\$ / 1000 request, to make 2M \$ they need to serve 2M / 5 \* 1000 = 400k requests total (for reference, Google handles 10B requests per day). So per day Exa needs to handle (400k/365) = ~1100 requests per day to break even in terms of infra cost which can be handled with just 100 peak RPS (11 RPS = 1M request / day, 33 RPS = 3M request / day. Assume 3x for peak load)

Each 1000 request costs 5\$
If you're using Exa at 5 QPS,

If you wanna saturate 5QPS, you'll do 1000 reqeusts in 1000 req / 5req/sec = 200s and it will cost 5 \$.

There are 86400 seconds in a day, so one client can at max spend 86400 / 200 _ 5 = 432 _ 5 = 2160 \$ per day.

5 QPS => 450k requests / day (11 RPS = 1M req / day)

If Exa has 10 such clients, they can make 22k per day or .

Exa just raised 85M$\ and at a 700M$ valuation, assuming 60-120x multiple (high but possible in a super hot AI sector, perplexity is at 180x, HF is at 90x, Cohere is at 250x), they might be making 5-12M\$ in ARR

5M ARR means 14k\$ / day

14k\$ / day means, they serve 31 RPS

To serve 31 RPS with p50 of 420ms, you just need 13 cores (todo: a lot of this is just GPU + IO + network time (they say 50ms), so factor that as well)

Exa AI python SDK downloads are at 30k / day. Assume only 0.01% converts to paying user, that means 3 new users daily (is this a fair assumption?)

## Search latency analysis & CPU requirements:

[Exa benchmarks](https://exa.ai/blog/fastest-search-api) say their p50 = 420ms and p95 = 600ms.

Out of this 420ms, we have 3 types of components:

- Network latency: They say it's 50ms. So we have 420-50ms = 370ms
- Embedding generation: I'm assuming 60ms with 32 batch size based on [this](https://github.com/huggingface/text-embeddings-inference)
- Re-ranking with cross encoder models: Let's assume another 140ms
- Vector search + Text + Re-ranking with uncompressed vectors. remaining: 370-60-140=170ms

Let's say search finishes in 20ms. How much disk can you read in 150ms? You can read 8KB pages (= one fp16 4096 dim vector) randomly, you can read 1MB in 15ms. This means 1000 / 8 = 125 vectors in 15ms. So when you have 150, you can read 150/15\*125=1250 vectors randomly. This makes sense, we can read 1000 original vectors from disk. Each of the 10 clusters could return 100 items each and then we rerank them. This sounds reasonable.

So overall vector search must be like:

- Created IVF Index with 10B BQ + MRL truncated vectors that are split in 100k clusters with 100k points each
- Convert query into embedding: 30ms
- Search: 100k centroids comparison + 100k points vector search in 10 clusters in parallel => 10 \* 100 results from each
- Rerank 1: Rerank 1000 vectors by original reading vectors from disk (180ms)
- Rerank 2: Cross encoder models: Probably a cross-encoder model

5M 1536 => 4096, 2M op/sec

let's assume batch size is 32,

100M vectors, 512 bytes/vector (fp16 256), 1TB/memory bandwidth (must have connected multiple devices??)

50 GB to read, 1 TB / sec

1000 / 50 => 20 times / sec. Or each full memory read takes, 50ms

B >= QPS \* full_scan_latency

100 / 20 = 5

Related to https://www.youtube.com/watch?v=jlMAX2Oaht0

. Compute comes at 10 \$ / month

If you assume 10B vectors, 100k clusters, each with 100k vectors. You search in top `0.01%` of these 100k clusters = 10 clusters. So 2nd step has go parallely search in 10 clusters each of size 100k

100k + 1.2 _ 100k = 2.2 _ 100K. Assuming 100K takes 5ms. This should be finished within 12ms

Then you need to read 1M 8KB (8GB) vectors from disk randomly, each takes 100 us => 1M \* 0.1 ms = 100k ms => 100s (clearly wrong)
If same thing was sequential, it would have take 1us for each and hence 8GB could be read in 1s (hmm, this contradicts with numbers in napkin maths throghtput side. why?)

Remaining

Search is more IO bound than compute bound, so I'm assuming out of 500ms, only 250ms is spent in embedding & lexical/vector search and we want only 70% CPU utilization and add 1.5x for peak load safety.

```py
cpu_cores = (RPS * actual_cpu_time_per_req) / (target_cpu_util * 1000ms) * 1.5
# For 100 RPS (10M request / day)
(100 * 50) / (0.7 * 1000) * 1.5  = 10.7 cores # nice
```

Each core costs 10 \$ if you reserve, so cost of CPU should be just 107\$ / month to serve 100 RPS

### What if Exa used HNSW instead of IVF Index? (TODO: Clean up and back with numbers)

- I know HNSW returns 10 results within 3ms with 1M vectors with SQ with 97% recall. The search time complexity of HNSW is `log(N)` while IVF is `log(sqrt(N))`. This means to go from 1M to 10B, IVF grows by `math.sqrt(1e10) / math.sqrt(1e6) = 100x` while HNSW search time grows only by `log(1e10) / log(1e6) = 10/6 = 1.6`. So it should remain within 5ms (Let's take 25ms assuming other parts don't scale equally well) if everything scales while IVF index search could reach 500ms (which is similar to what we see in numbers reported by Exa benchmarks as 420ms is p50).
- However, the cost of building and storing index will shoot up which makes it tough for Exa since they have regular updates from the web.
- Something similar is also proven in [this blog by Vespa](https://blog.vespa.ai/billion-scale-knn-part-two/)
- If HSNW was running at 200B scale (Perplexity), that would be `3 * math.log(200 * 1e9) / math.log(1e6) = 3 * 1.88 ~= 6ms`

## Conclusion:

In search you have to pick two between cost, quality, and speed. Exa.ai seems to optimize for quality & speed which is great for the use cases they want to serve! They also offer Websets that go deeper into the web and use LLMs to filter out documents, it optimizes mainly for quality because speed and cost and expected to to be high.

Thanks to my friend [Nirant Kasliwal](https://in.linkedin.com/in/nirant) for reviewing the blog.

### References:

- https://exa.ai/blog/building-web-scale-vector-db
- https://exa.ai/blog/bm25-optimization ->
- https://exa.ai/blog/scaling-our-highlights-server -> TODO
- https://exa.ai/blog/evals-at-exa -> TODO
- https://www.perplexity.ai/search/what-s-the-cost-of-running-per-9IfuBvj2RnCPwwoTlMGyoA#0
