---
title: 'Search Internals: Exa AI'
date: '2025-09-16'
lastmod: '2025-09-16'
tags: ['search', 'internals']
draft: true
summary: ''
authors: ['default']
---

There are multiple parts of Exa AI architecture that they have described in their blogs. I've compiled them together and added some of my own notes and lots of assumptions for some napkin maths as a fun exercise.

### Text search:

- **BM25 index** costed them 1.8TB RAM (1.8 \* 10^12 bytes) for 1B (10^9) billion docs initially.
  - After [this post](https://exa.ai/blog/bm25-optimization) it dropped to 1.2TB (-50%) RAM per 1B docs.
  - As per [napkin maths](https://github.com/sirupsen/napkin-math), this means 2$ / month / GB \* 1200 GB = 2400 $ / month
  - I'm gonna assume they have 10B documents, because they clearly don't scrape the whole web while [Google holds 400B+ pages](https://zyppy.com/seo/google-index-size/), [Fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) has 26B docs.
  - With 10B docs, monthly bill for just the BM25 index part (not accounting for actual documents) would be $2400 \* 10 = 24k / month => ~300K USD / year

### Vector search:

- Trained custom Matryoshka embeddings with 4096 dims but only used 256 dims in memory (i.e. MRL truncation). I'm assuming it might be a fine-tuned version of https://huggingface.co/Qwen/Qwen3-Embedding-8B which fits the description well.
- Used binary quantization (BQ)
- Precomputed possible outputs of BQ computations in CPU registers (instead of RAM) for similarity computation. Very cool trick!
- Re-ranking with original vectors to compensate for all this approximation.
- Each uncompressed vector would take 4096 dims _ 4 bytes = 16KB by default. This means 1B vectors would take 16 _ 10^3 bytes _ 10^9 = 16 _ 10^12 = 16 TB of disk storage for 1B original vectors.
  - By truncating matryoshka it becomes, 16 / (4096 / 256) TB = 16 / 16 TB = 1 TB for 1B vectors.
  - With binary quantization, 4 bytes (32 bits) becomes 1 bit. So 1 TB / 32 = ~32 GB per 1B vectors.
  - With BQ + MRL truncation and assuming their vector index is 1.25x, 32*1.25 = 40GB per 1B vectors. So overall, 40GB * 2$ / month / GB = 80$ / month (Huh, something is wrong)
  - This is much less than what BM25 costed them.
  - https://huggingface.co/Qwen/Qwen3-Embedding-8B has massive context length of 32K tokens which is huge. As per this quick [research](https://www.perplexity.ai/search/average-words-per-page-on-the-qjaJmyDVQ823yG.lh.Ba3g#0), most internet pages and even research papers would fit in this context length. So chunking isn't required. 1 document = 1 vector.
  - I think they build multiple vector indices. For example they might be using,
    - MRL truncation + re-ranking with original vectors for their "neural search"
      - Storage cost: 1TB \* $0.35/GB	for [regional SSD](https://github.com/sirupsen/napkin-math) = 350$ / month per 1B => 42k USD / year for 10B docs.
      - Memory Cost: 1024 GB \* 2$/GB ~= 2000$ / month / 1B vectors => 240K USD / year for 10B docs
    - BQ + MRL truncation + re-ranking with original vectors for their "fast search" (32x less mem usage & slightly faster)
      - Storage requirements: 32GB FOR 1B vectors
      - Memory Cost: 240K / 32 = 7.5K USD / year for 10B docs (yeah, BQ cost savings are insane)

## Metadata storage:

- Each original vector is 16KB, but we also need to store metadata like title, url, publishedDate, author, id, image, favicon, text, summary, etc. I'm assuming it's gonna take another 4KB. (Why? because their [get content API example](https://docs.exa.ai/reference/get-contents) returns page with size 3KB). So total metadata storage on disk is 16TB / 16 _ 4KB = 4TB for 1B doc metadata. This means 4000 GB _ 0.35$ / GB = 1.4k USD / month for 1B docs => 170k USD / year for 10B docs metadata.

### Content storage:

## Extra features:

### Highlights:

-

### Filtering:

- For each filterable value (category, date, etc), they create inverted indexes. These are hashmaps from the filterable value to a list of document IDs that match the filter. If the user specifies a filter, then they only run their search algorithm over the documents that match the filter in the inverted index.

### Evals:

-

## Overall costs:

Component | Storage type | Cost per GB | Cost for 1B docs | Cost for 10B docs |
Content storage | S3 | x | y | z |
Vector storage | Disk | x | y | z |
Metadata storage | Disk | x | y | z |
Posting lists index for BM25 | RAM | x | y | z |
Vector index for "neural" search | RAM | x | y | z |
Vector index for "fast" search | RAM | x | y | z |

In search you have to pick two between cost, quality, and speed. Exa.ai seems to optimize for quality & speed which is great for the use cases they want to serve! They also offer Websets that go deeper into the web and use LLMs to filter out documents, it optimizes mainly for quality because speed and cost and expected to to be high.

References:

- https://exa.ai/blog/building-web-scale-vector-db
- https://exa.ai/blog/bm25-optimization ->
- https://exa.ai/blog/scaling-our-highlights-server -> TODO
- https://exa.ai/blog/evals-at-exa -> TODO
