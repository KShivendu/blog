---
title: 'I Reverse-Engineered Exa.ai Infrastructure Cost with Napkin Math'
date: '2025-09-16'
lastmod: '2025-10-02'
tags: ['search', 'internals', 'infra']
draft: true
summary: ''
images: ['/static/images/blogs/exa-clean.jpg']
authors: ['default']
---

![Exa Napkin Math Blog Cover Image](/static/images/blogs/exa-clean.jpg)

> Looking for TL;DR (Too Long; Didn‚Äôt Read)? Check [key takeaways](/blog/exa-napkin-math#key-takeaways)

[Exa](https://exa.ai/) is a cool web search engine company. They recently [raised 85M\$ for Series B](https://exa.ai/blog/announcing-series-b). There are multiple parts of Exa AI architecture that they have described in their [blogs](https://exa.ai/blog). I've compiled them together and added some of my own notes and lots of assumptions for infra cost [napkin math](https://www.youtube.com/watch?v=IxkSlnrRFqc) as a fun exercise for myself. Please **take my estimates with a grain of salt** since my goal is to just get in the right ballpark. Also feel free to DM me on [X](http://kshivendu.dev/x) or [Linkedin](http://kshivendu.dev/linkedin) if you see any wrong or suboptimal assumptions :)

Update: Exa recently did [improvements](https://exa.ai/blog/exa-api-2-0) across their pipeline to make search faster and more relevant. This means some of my assumptions might be slightly outdated now, but the fundamentals are still applicable. So enjoy!

## üì¶ Content Storage

Let‚Äôs estimate the storage requirements for large-scale text datasets:

- [Fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)
  - 26B documents occupy 44 TB (compressed), meaning 1B docs ‚âà 1.69 TB (compressed)
  - It has 18.5T tokens and used GPT-2 tokenizer. Assuming the standard rule of 1 word = 1.3 tokens, this means 26B docs ~= 14T words.
    - 1 doc ~= 711 tokens ~= 538 words.
    - English words on [avg](https://www.wyliecomm.com/whats-the-best-length-of-a-word-online/) have 5 chars and 1 char = 1 byte, so 538 \* 5 bytes ~= 2.7KB / doc
    - 1B docs ~= 2.7TB (estimated uncompressed)
- [English Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Statistics#Articles)
  - 7M docs occupy 24 GB (compressed), 1B docs ~= 3.42 TB (compressed)
  - It has 708 words / doc on avg. Assuming the standard rule of 1 word = 1.3 tokens, this means 920 tokens / doc on avg.
    - 1 doc ~= 920 tokens ~= 708 words
    - 708 \* 6 bytes ([reason](https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#Number_of_words)) = 4248 bytes
    - 1B docs => 4.2TB (estimated uncompressed)

Given these, let‚Äôs assume Exa‚Äôs crawlers curate, sanitize, and create summaries pages (with [LLMs](https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/)) efficiently enough such that:

- They are similar to [Fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) but a little more dense.
- 1B docs require 3TB (uncompressed) and 1.9TB (compressed)
- On avg **1 page ~= 600 words ~= 780 tokens ~= 3KB (uncompressed) ~= 1.9KB**

| Resource          | Rate       | Volume for 1B docs | Cost/Month |
| ----------------- | ---------- | ------------------ | ---------- |
| **S3 storage**    | $0.02 / GB | 1.9 \* 1e3 GB      | $38        |
| **Ephemeral SSD** | $0.08 / GB | 1.9 \* 1e3 GB      | $152       |
| Total             |            |                    | $190       |

## üîç Lexical/Keyword search (BM25):

- They use the standard [BM25 algorithm](https://www.youtube.com/watch?v=ziiF1eFM3_4&t=698s) for lexical search
- Initially, this required about 1.8 TB RAM for 1B docs
- After optimizations in [this post](https://exa.ai/blog/bm25-optimization) it **dropped to 900GB (-50%)** RAM for 1B docs

### Napkin math

- Most of storage overhead in BM25 comes from the postings lists. Other data structures have relatively negligible impact.
- As per [their blog](https://exa.ai/blog/bm25-optimization#:~:text=a%20single%20byte.-,Assuming%20the), each posting list entry takes:
  - doc_id ‚Üí 4 bytes (uint32)
  - term_freq ‚Üí 1 byte (fp8)
  - 5 bytes total
- This means 1.8 TB / 5 bytes = **360 tokens are unique in a doc** on avg.
  - It's 44% of the 780 tokens per doc that we assumed.
  - This percent is kinda close to the standard [type-to-token ratio](https://www.sketchengine.eu/glossary/type-token-ratio-ttr/) in high quality pages.
- There would be a tiny overhead of vocab size
  - Even if we assume 10M unique vocab terms across a 1B corpus with each word being of 5 letters ([source](https://www.wyliecomm.com/whats-the-best-length-of-a-word-online/)), it would be just 5 \* 10M \* 1 byte / char = 50MB
  - And note that most of this comes from unique terms accounting for misspellings, languages, and domain specific terms.
- Note that Exa mentioned they used `uint32` for doc ID that means they must be having fewer than 4.2B docs (2^32) when they wrote [this blog](https://exa.ai/blog/bm25-optimization) in May 2025. They started used [delta encoding](https://exa.ai/blog/bm25-optimization#:~:text=2.%20Variable%2Dlength%20%2B-,Delta%20Encoding,-for%20Document%20IDs) so it will scale smoothly when they switch to `uint64` for holding 10B docs (probably happened in Exa 2.0 [launch](https://exa.ai/blog/exa-api-2-0#:~:text=now%20crawl%20%2B%20parse-,tens%20of%20billions,-of%20webpages%20and)).
- I'm assuming that 100% of index is kept in RAM for faster iteration on posting lists (~2-5x faster compared to sequential reads in SSD - [source](https://github.com/sirupsen/napkin-math))

| Resource | Rate      | Volume | Cost/Month for 1B docs |
| -------- | --------- | ------ | ---------------------- |
| RAM      | $2 / GB   | 900GB  | $1800                  |
| SSD + S3 | $0.1 / GB | 900GB  | $90                    |
| Total    |           |        | $1890                  |

## üß≠ Vector search:

<div align="center">
  <img src="/static/images/blogs/exa-vector-search-pipeline.svg" alt="Exa vector search pipeline" />
</div>

- As per [this](https://exa.ai/blog/building-web-scale-vector-db), they use [binary quantization](https://qdrant.tech/articles/binary-quantization) (BQ) and some clever CPU tricks for vector distance calculations ([SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) + precomputing possible outputs and keeping them in CPU registers for skipping calculation).
- They train [Matryoshka embeddings](https://huggingface.co/blog/matryoshka) with 4096 dims but only keep first 256 dims BQ vectors for index
- They use [IVF Index](https://youtu.be/chz74Mtd1AA?si=DvvDE3rO9HvVvox7&t=429) (also used in FAISS & pgvector) which an Approximate Nearest Neighbours Algorithm. It basically relies on k-means clustering, and during search time, finds the relevant clusters by comparing distance with respective cluster centroids, and only queries few of those clusters.
- Then they do 1st re-ranking with 1024 dims BQ vectors and select top candidates to pass to the next stage.
- Lastly, they do 2nd re-ranking with 2048 dims f16 vectors to compensate for all this approximation/compression.
  - Note that model originally generated 4096d vectors. They probably don't use the full 4096 at least in Exa Fast search

### Napkin math

- I think their model is a fine-tuned version of [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B) since it fits the [description](https://exa.ai/blog/building-web-scale-vector-db#:~:text=4096%20floating%2Dpoint%20numbers) well and is very popular.
  - Qwen3-Embedding-8B has a massive context length of 32K tokens. As per this quick [research](https://www.perplexity.ai/search/average-words-per-page-on-the-qjaJmyDVQ823yG.lh.Ba3g#0), most internet pages and even research papers would fit in this context length. So chunking isn't required. 1 document = 1 vector.
  - Update: With Exa 2.0 [launch](https://exa.ai/blog/exa-api-2-0), they have pre-trained and finetuned the embedding model and claim to have found new embedding techniques. However, I don't expect the architecture to be wildly different from state-of-the-art OSS models like Qwen.
- Let's estimate the storage requirements of vectors
  - Each original vector would take 4096 dims \* 2 bytes (assuming fp16) = 8KB by default. This means 1B vectors would take 8 \* 10^3 bytes \* 10^9 = 8 \* 10^12 = 8 TB for 1B docs.
  - By truncating matryoshka to 256 dims it becomes, 8 / (4096 / 256) TB = 8 / 16 TB = 512 GB for 1B vectors.
  - With binary quantization, 16 bits (since fp16) becomes 1 bit. So 512 GB / 16 = 32 GB per 1B vectors or **32 bytes (256 bits) per vector**
  - Note how MRL truncation + BQ led to 16\*16=256x reduction
- For IVF index with MRL truncated 256d BQ vectors for their "neural search"
  - IVF Index needs centroids generated from k-means clustering algos, If they have 20B vectors, they should have **200k clusters, each having 100K vectors** since it becomes very hard to brute-force compare vectors in segments larger than 100k within their latency budget.
  - Each cluster will have a centroid and I'm assuming they compare with uncompressed centroid vectors for maximizing recall.
  - Centroids will require 200k \* 4096 \* 2 bytes (f16) ~= 1.56 GB
  - Each vector ID needs to be stored in the IVF Index, 20B \* 8 bytes = 160 GB. Note that I used 8 bytes because it must be `uint64` to store 4.2B+ docs.
  - Each 256d truncated + BQ compressed vector will take 32bytes (256 bits), so 20B \* 32 bytes = 640 GB
  - So total requirements are like 1.56 + 160 + 640 GB ~= 800 GB for 20B vectors
    - This is 40GB for 1B vectors
    - Index size is just 40/8000\*100 = 0.5% of the full vectors (nice!)
- For 1st stage re-ranking with 1024 dim MRL truncated BQ vectors:
  - Storage for 1B vectors takes 128GB (1024 / 8 \* 1B / 1e9)
- For 2nd stage re-ranking with 2048 f16 vectors:
  - Storage for 1B docs takes 4000GB (8TB/2). This is okay for disk but storing all of them in RAM would be too costly.
  - However, Pareto principle says 20% of docs will account for 80% queries so let's cache only 25% (1000 GB) in RAM
- Some of these components must be cached in RAM for fast queries and all the components must be kept on local (ephemeral) disk for paging in/out of memory quickly and handle a node crash. The ultimate source of truth can be S3 which is regularly updated to keep the index, vectors, and content fresh.

| Component                        | Rate      | Volume | Cost/Month for 1B docs |
| -------------------------------- | --------- | ------ | ---------------------- |
| Index RAM                        | $2 / GB   | 40GB   | $80                    |
| Index Ephemeral SSD + S3         | $0.1 / GB | 40GB   | $4                     |
| 1024d BQ Vectors in RAM          | $2 / GB   | 128GB  | $256                   |
| 1024d BQ Vectors in SSD + S3     | $0.1 / GB | 128GB  | $12.8                  |
| 2048d Vectors RAM                | $2 / GB   | 1000GB | $2000                  |
| 2048d Vectors Ephemeral SSD + S3 | $0.1 / GB | 4000GB | $400                   |
| Total                            |           |        | $2752                  |

Note how this is 2.8k / 1.9k = **~1.5x of lexical search** and **87% of it comes from storing the original vectors in RAM for re-ranking**.

## üóÇÔ∏è Metadata storage:

- For each web page, they also need to store metadata like title, url, publishedDate, author, id, image, favicon, text, summary, etc.
- Their [search API example](https://docs.exa.ai/reference/search) returns page attributes that can be stored in < 300 bytes. It's a tiny overhead so let's assume 512bytes (0.5KB)
- So total metadata storage on disk + S3 is 1B docs \* 0.5KB = 0.5TB for 1B doc metadata
- Assuming that metadata for the same 25% points are cached, we need 125GB of RAM

| Component                   | Rate      | Volume | Cost/Month for 1B docs |
| --------------------------- | --------- | ------ | ---------------------- |
| Metadata RAM                | $2 / GB   | 125GB  | $250                   |
| Metadata Ephemeral SSD + S3 | $0.1 / GB | 500GB  | $50                    |
| Total                       |           |        | $300                   |

## üåêüì§ Network egress costs:

The APIs respond back to the user and that also costs money. [Napkin math](https://github.com/sirupsen/napkin-math) says it's `$0.1` per GB for "internet egress"

I did a basic search API call with Exa and response size was around 3KB. If they serve, 300M requests / month (only 110 RPS required, with peak 300 RPS provisioned), it would cost them

3 KB \* 300M => 900 GB of egress => 900 \* 0.1 \$ = 90 \$ / month for 300M requests which is very cheap!

## üî¢ Embedding generation:

- As we [previously](#-content-storage) derived/assumed, let's say each Exa doc has 780 tokens.
- If they were to embed 1B docs with OpenAI `text-embedding-3-large` embedding in batches (see [pricing](https://platform.openai.com/docs/pricing#embeddings))
  - 780 \* 1B \* 0.065 \$ / M token = 50.7k \$ for 1B docs (insane!)
  - Note that OpenAI models aren't great for retrieval and clearly are very costly
- As per [this blog](https://www.tensoreconomics.com/p/why-are-embeddings-so-cheap#:~:text=a%20million%20tokens.-,In%20Tab.%202), assuming 100% utilization with H100 GPUs rented at \$ 2 / hr, the cost is $ 0.0166 / 1 M tokens.
  - 780 \* 1000 \* 0.0166 \$ / M token ~= 13k $ for 1B docs.
  - This is much better! However, notice that this still surpasses all the costs.
  - Exa realised this early and that's why they bought/built their [Exacluster](https://exa.ai/blog/meet-the-exacluster) to save massively on costs:
    - One of the cheapest H100 is [offered](https://vast.ai/pricing) at 1.87\$ / hr. But general pricing is 2-4\$ / hr.
    - Assuming a worse case margin for 20% for the above rate, we get 1.5\$ / hr. This means embedding price is 0.0166 / 2 \* 1.5 = 0.0125 \$ / 1M tokens when owning H100s.
    - Exacluster has H200s & A100 instead of H100s. But let's assume the same rates.
    - Note that the Exacluster is also used for training embedding model and re-ranker so there's more value to be derived from the Exacluster.

| Embedding model               | Rate             | Tokens    | Cost for 1B docs |
| ----------------------------- | ---------------- | --------- | ---------------- |
| OpenAI `text-embedding-small` | $0.0650 / 1M tok | 780 \* 1B | $50.7k           |
| Qwen on Rented H100 GPUs      | $0.0166 / 1M tok | 780 \* 1B | $13k             |
| Qwen on Exacluster            | $0.0125 / 1M tok | 780 \* 1B | $9.75k           |

They are **saving at least 3.25k$ / 1B docs** by building Exacluster which is great! And with **20B docs it would save 65k USD annually**. The savings will increase further with crawling updates.

## Re-ranker:

- They might be using something like the [Qwen Rerankers](https://huggingface.co/Qwen/Qwen3-Reranker-0.6B#qwen3-embedding-series-model-list)
- Public [providers](https://deepinfra.com/Qwen/Qwen3-Reranker-0.6B) charge \$ 0.010 \/ 1M tokens for the 0.6B model. Let's assume the same rates.
- It compares 1 query (same 10 tokens) against 100 documents (each with 780 tokens avg). Total tokens: 100 \* (780 + 10) = 79k
- Note: This is purely a **search time cost** unlike document embeddings which have to be stored/maintained beforehand.

| Re-ranker model       | Rate             | Tokens / query | Cost of 1k queries |
| --------------------- | ---------------- | -------------- | ------------------ |
| `Qwen3-Reranker-0.6B` | $0.0100 / 1M tok | 79k            | 0.790 \$           |
| `Qwen3-Reranker-4B`   | $0.0250 / 1M tok | 79k            | 1.975 \$           |
| `Qwen3-Reranker-8B`   | $0.0500 / 1M tok | 79k            | 3.950 \$           |

Note that Exa charges 5\$ / 1k queries so it makes more sense for them go with something like `Qwen3-Reranker-0.6B` at least in Exa fast mode.

## üï∑Ô∏è Crawling updates

- As per [this research paper](https://snap.stanford.edu/class/cs224w-readings/Fetterfly03PageEvolution.pdf), web content changes meaningfully. 25% of pages show significant textual changes every month.
- There might be a single digit **monthly growth of the dataset - say 2%** (assuming aggressive crawling)
- Content disappearance: ~0.5% monthly
- Overall let's assume **30% monthly overhead** if Exa optimizes aggressively for freshness.

## üí∞üìä Overall costs:

I'm assuming that Exa now has 20B docs with their [Exa 2.0 launch](https://exa.ai/blog/exa-api-2-0#:~:text=now%20crawl%20%2B%20parse-,tens%20of%20billions)

| Component        | Cost for 1B docs (monthly) | Cost for 20B docs (one-time setup) | 30% overhead for 20B docs (monthly) |
| ---------------- | -------------------------- | ---------------------------------- | ----------------------------------- |
| Content storage  | 0.19k                      | 3.8k                               | 1.14k                               |
| Lexical storage  | 1.89k                      | 37.8k                              | 11.34k                              |
| Vector storage   | 2.75k                      | 55k                                | 16.51k                              |
| Metadata storage | 0.30k                      | 6k                                 | 1.80k                               |
| Embedding cost   | 9.75k                      | 195k                               | 58.50k                              |
| Egress           | 0.09k                      | 1.8k                               | 0.54k                               |
| **Total**        | 17.37k                     | 300k                               | 90k                                 |

Notes:

- **65% of the cost comes from just embedding generation**.
- One-time **bare minimum setup cost for 20B docs is 300K$**
- **Yearly cost of maintaining Exa 2.0 index with 20B+ docs ~= 90k \* 12 ~= 1.1M \$**
- And I haven't even considered the cost of clustering/indexing, crawling, LLM based parsing. But it should be all done within 1.5-2M$ / year for 20 B docs.
- Furthermore, there's will be more cost of running Exacluster, runtime CPU + GPU (query embedding/re-ranking), LLMs for websets, observability, and rest of the infra.
- Perplexity operates at **200B scale** and Exa would probably reach this scale soon with their latest funding, so their cost would shoot up to **15-20M\$ / year just for search part**. This makes sense given their latest funding of 85M\$, they need lots of capital for few years of runway.

## ‚è±Ô∏è Search latency analysis:

### Network latency:

I queried Exa AI DNS records and looks like they are only present in AWS `us-west-2` region. This has downsides because it increases latency a lot. The round trip time from India is `600-800ms`. You can find ping times from all over the world [here](https://globalping.io/?measurement=g9UVVPjZD7g3OmqV). There are two solutions to this:

- Expand into new regions. But it's very costly to replicate whole infra in different region.
- Use something like Cloudflare Edge network. It basically proxies the requests through Cloudflare edge network which is faster and more stable than traditional internet route. It's not zero overhead, but is definitely a big improvement.

Update: Looks like Exa has switched to Cloudflare Edge like Perplexity with Exa 2.0. This means the latency across the globe has decreased. I see that now the round trip latency for empty request from India is at `300-400ms`. Good job, Exa team!

You can also use these commands to get rough idea of latency:

```sh
time curl --head https://api.exa.ai/search # from your machine
globalping http api.exa.ai from World --host api.exa.ai --path '/search' --limit 20 --latency # globally
```

### Lexical search latency

The [Exa blog](https://exa.ai/blog/bm25-optimization) clearly says they get sub 500ms latency (p50/median) for retrieving top 1000 docs with lexical search. Even if they retrieve top 10 or 100, it would just slightly faster because you need to rank the same documents. This is why they seem to **only use neural search in their fast mode**.

To add, it's a known fact that BM25 latency degrades faster than vector search with:

- Longer queries (More posting lists to scan. This downside can be saturated using [wAND](https://izihawa.github.io/summa/blog/how-search-engines-work) but you still see more variance in latency)
- More data (less pronounced. But this happens especially if you get common but non-stopword terms)

For example effect of longer queries part can be seen in [this benchmark](https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-4/#:~:text=Benchmarking%20Results):

| Retriever      | Average latency (ms) | p999 latency (ms) |
| -------------- | -------------------- | ----------------- |
| Lexical (WAND) | 16                   | 209               |
| Vector (HNSW)  | 18                   | 28                |

You can see how avg latency is similar for both vector search and lexical search but p999 is 7.5x more for lexical search. Exa uses IVF Index not HNSW so it would also vary more but I expect BM25 latency to be affected more.

Also, if Exa operates with 20B+ docs (with Exa 2.0), I can imagine that they split 20B docs into multiple "posting list shards" and query them in parallel and combine results afterwards.

### Neural search latency (Exa fast)

[Previously](https://exa.ai/blog/fastest-search-api) Exa fast search had p50 = 425ms and p95 = 604ms.

[Now](https://exa.ai/blog/exa-api-2-0) they improved it to p50 = 346ms and p95 = 462ms.

Out of this 425ms, they **probably** had these components:

![Exa Search Pipeline](/static/images/blogs/exa-search-pipeline.svg)

- Network latency: They say it's 50ms. So we have 425ms-50ms ~= 375ms remaining.
- Embedding generation with as per [this](https://www.tensoreconomics.com/p/why-are-embeddings-so-cheap#:~:text=despite%20batch%20size%20increasing) with batch size = 1:
  - As per the blog, there's very little benefit from batching in embedding models.
  - 260 tokens with batch size = 1 => 37RPS => 27ms
  - 260 tokens with batch size = 8 => 64RPS => 15ms
  - Let's assume with 10 tokens (7 word query), we get 70RPS and hence 14ms latency. Assume 15ms for cleaner numbers. Remaining: 375ms-15ms = 360ms
- Re-ranking queries-docs with cross encoder models: Let's assume another 150-200ms because it's the standard for ranking 100 items against a query. Remaining: 360-200=160ms.
- Vector search + 2 stage re-ranking with uncompressed vectors should be done in around 160ms.
- With Exa 2.0 fast mode, they claim a p50 diff of 79ms and p95 diff of 142ms. This must come from improvements in re-ranking model or vector search part.
  - They might have been able to afford a distilled or quantized version of the cross encoder re-ranker step because of fine-tuning without hurting precision.
  - Or they might be doing less re-ranking steps with less compressed vectors in the vector search step.

## What if Exa used HNSW instead of IVF?

- In [this billion-scale KNN benchmark](https://blog.vespa.ai/billion-scale-knn-part-two/), a 72 vCPUs machine handles 1B 100dim BQ vectors:

  - Search latency of HNSW: 2ms with recall 50% and 4ms with 90% recall with k=10
  - Realtime indexing throughput: 15.4k WPS
  - So avg write latency: 72 / 32_000 = 4.68ms. This is just a little higher than search because HNSW mainly needs to search and then it knows where to add new links

- The avg emperical (not guaranteed) search time complexity of HNSW is `log(N)` while IVF is `log(sqrt(N))`. In theory, it means that to go from 1M to 10B docs
  - IVF latency balloons by `math.sqrt(1e10) / math.sqrt(1e6) = 100x`
  - HNSW search time only increases by `log(1e10) / log(1e6) = 10/6 = 1.7x`
  - If we assume they perform similar at 1M scale - say 2ms with single thread. HNSW should remain around 5ms while IVF index search would take 100-200ms.
  - Note that these numbers become worse when the system has more RPS than the number of threads available because of context switching.
  - This is why Exa would have to split 10B docs into smaller 100k clusters and query only 10 of them (limited number of threads) in parallel and that's degrading recall to minimize latency.
  - Also as previously highlighted, in IVF, the cluster sizes vary and hence like BM25, the query latencies (avg latency vs p999) would vary more depending on the query.
- Similar flaws of scaling IVF were also demonstrated in [my previous benchmark blog with Nirant](https://nirantk.com/writing/pgvector-vs-qdrant) that compared Qdrant's HNSW and PGVector's IVF Index and our blog accelerated the implementation of HNSW in pgvector.
- If Exa switched to HNSW, the cost of building and storing index would rise because of higher RAM and CPU requirements. But the recall and latency of HSNW will be much better. Perplexity uses HNSW and this is part of the reason why they were able to claim lower latency and higher recall against Exa 1.0 in their [recent benchmarks](https://www.perplexity.ai/api-platform/resources/architecting-and-evaluating-an-ai-first-search-api#:~:text=community%20as%20well.-,Search%20API%20Latency).
- Update: Exa has scaled to [tens of billions](https://exa.ai/blog/exa-api-2-0) of records (assumed 20B) with Exa 2.0 launch and seems to be doing better or similar to Perplexity without doing HNSW because they probably did optimizations in re-ranking layers as I [highlighted above](https://kshivendu.dev/blog/exa-napkin-math#:~:text=they%20claim%20a-,p50%20diff%20of%2079ms,-and%20p95%20diff).

### Key takeaways:

- Exa applies heavy compression through matryoshka embeddings and binary quantization, then offsets the loss with uncompressed vectors and re-ranking.
- In the query pipeline, vector search requires careful planning but takes less time and cost while re-ranking causes more cost and latency.
- Documents on the web change over time and cost of re-indexing (document embedding generation + k-means clustering) is high. To mitigate this, Exa built their ExaCluster.
- Creating a large web scale index (with 200B docs) is not so cheap, 3M \$ one-time bare minimum setup cost + ~15-20M \$ \/ year for maintaining it.

## Closing remarks:

Web search started in 1994 with [Yahoo directories](https://en.wikipedia.org/wiki/Yahoo_Directory), then search engines like Alta Vista introduced Keyword search using [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) in late 1990s, and then eventually Google came in 1998 and dominated the game with [Pagerank](https://en.wikipedia.org/wiki/PageRank). Over the next two decades, Google added lots of heuristics and statistics based approaches. Then around 2015 they started integrating ML models such as RankBrain and BERT, enabling semantic understanding and context-aware ranking.

And very recently, we see that even those are being [surpassed](https://exa.ai/blog/evals-at-exa) by more generalizable LLMs. Infact, now you can ask more [complex queries](https://exa.ai/blog/websets-evals) to the web which were impossible with previous techniques. It's literally [Bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) 101 which says approaches that scale with compute beat human designed heuristics.

We‚Äôre entering an era where startups are recreating Google scale search and number of searches will rise exponentially because of AI agents driving majority of search request. That's why products like Exa, Parallel, and Perplexity are effectively "caching" the web and serving "reads" in an optimized way. While others like Perplexity Comet, OpenAI Operator, etc want to "write" to the web (i.e. take actions). **Web and search are changing fast** and I'm very excited about their futures!

## Acknowledgements:

Thanks to my friend [Nirant](https://in.linkedin.com/in/nirant) for reviewing the blog.

- https://github.com/sirupsen/napkin-math
- https://www.tensoreconomics.com/p/why-are-embeddings-so-cheap
- https://www.youtube.com/watch?v=y72AhP4oLZE
- https://exa.ai/blog/building-web-scale-vector-db
- https://exa.ai/blog/bm25-optimization
- https://exa.ai/blog/fastest-search-api
- https://exa.ai/blog/evals-at-exa
- https://exa.ai/blog/websets-evals
